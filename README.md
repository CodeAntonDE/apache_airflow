# 1.Из каких основных частей состоит AirFlow

Apache Airflow состоит из нескольких ключевых компонентов, каждый из которых выполняет свою роль в организации, планировании и выполнении рабочих процессов:

Основные части Airflow
1. DAG (Directed Acyclic Graph)

Основная сущность Airflow - это DAG, то есть направленный ациклический граф, который определяет структуру и порядок выполнения задач (tasks) в рамках одного рабочего процесса.

2. Task (Задача)

Задача - это отдельная единица работы внутри DAG. Каждая задача реализуется с помощью оператора и выполняет конкретное действие (например, запуск Python-скрипта, выполнение SQL-запроса и т.д.).

3. Operator (Оператор)

Оператор определяет, что именно будет делать задача. Существуют различные типы операторов: PythonOperator, BashOperator, EmailOperator, а также специальные сенсоры (Sensor), которые ожидают определенного события.

4. Scheduler (Планировщик)

Планировщик отвечает за определение времени и порядка запуска задач в DAG согласно расписанию. Он отслеживает готовность задач к запуску и инициирует их выполнение.

5. Executor (Исполнитель)

Исполнитель отвечает за непосредственный запуск задач. В зависимости от выбранного типа (например, LocalExecutor, CeleryExecutor, KubernetesExecutor) задачи могут выполняться локально, в распределённой среде или в Kubernetes-кластере.

6. Web Server (Веб-сервер) / WebUI

Веб-интерфейс Airflow позволяет пользователям управлять DAG'ами, запускать их вручную, отслеживать статус задач, просматривать логи и управлять настройками.

7. Metadata Database (База данных метаданных)

Центральное хранилище для всей служебной информации: статусов DAG и задач, расписаний, переменных, соединений и других метаданных. Обычно используется СУБД, совместимая с SQLAlchemy (PostgreSQL, MySQL и др.).

8. Worker (Рабочий процесс)

В распределённых конфигурациях (например, с CeleryExecutor) отдельные процессы-воркеры отвечают за выполнение задач, получая их от исполнителя.

Дополнительные компоненты
Sensor (Сенсор): специальный вид операторов, которые «ждут» наступления определённого события.

Connections (Соединения): объекты для хранения данных о подключениях к внешним системам (БД, API и т.д.).

Variables (Переменные): механизм хранения пользовательских настроек и конфигураций.


![image](https://github.com/user-attachments/assets/c8d8b4ea-55a4-48f6-920d-8ffb41ca0a76)

*************************************************************************************************************************************************
# 2) Какие операторы Airflow вы знаете?

В Apache Airflow существует множество операторов, которые представляют собой строительные блоки для определения задач в DAG. Вот основные и часто используемые операторы:

**PythonOperator** - позволяет запускать произвольные функции на Python. Один из самых популярных операторов благодаря гибкости.

**BashOperator** - выполняет команды bash или shell-скрипты. Используется для простых команд и скриптов.

**PostgresOperator** - выполняет SQL-запросы в базе данных PostgreSQL, используя подключение Airflow.

**SSHOperator** - запускает команды на удалённом сервере по SSH с использованием заранее настроенного подключения.

**EmailOperator** - отправляет электронные письма (часто используется для уведомлений).

**HttpOperator** - выполняет HTTP-запросы к API или веб-сервисам.

**DockerOperator** - запускает задачи внутри Docker-контейнеров.

**S3FileTransformOperator** - для работы с файлами в S3, например, преобразование данных.

**Transfer Operators (например, S3ToRedshiftOperator)** - предназначены для перемещения данных между системами, например, из S3 в Amazon Redshift.

**SlackAPIOperator** - отправка сообщений в Slack через API.

**SQLExecuteQueryOperator** - выполнение SQL-запросов в различных СУБД.

**YQExecuteQueryOperator** - оператор для выполнения запросов в Yandex Query.

Кроме того, существуют специализированные операторы от провайдеров, например, для работы с Airbyte API (AirbyteGeneralOperator) и другие, расширяющие функциональность Airflow.

Также стоит отметить, что начиная с Airflow 2.0 появилась возможность создавать операторы с помощью декораторов Python, что упрощает их использование и передачу данных между задачами.

Таким образом, Airflow предоставляет широкий набор операторов для различных задач: от запуска кода и команд до интеграции с базами данных, облачными сервисами и системами передачи данных. Это делает его универсальным инструментом для оркестрации рабочих процессов.





