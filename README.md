# 1.Из каких основных частей состоит AirFlow

Apache Airflow состоит из нескольких ключевых компонентов, каждый из которых выполняет свою роль в организации, планировании и выполнении рабочих процессов:

Основные части Airflow
1. DAG (Directed Acyclic Graph)

Основная сущность Airflow - это DAG, то есть направленный ациклический граф, который определяет структуру и порядок выполнения задач (tasks) в рамках одного рабочего процесса.

2. Task (Задача)

Задача - это отдельная единица работы внутри DAG. Каждая задача реализуется с помощью оператора и выполняет конкретное действие (например, запуск Python-скрипта, выполнение SQL-запроса и т.д.).

3. Operator (Оператор)

Оператор определяет, что именно будет делать задача. Существуют различные типы операторов: PythonOperator, BashOperator, EmailOperator, а также специальные сенсоры (Sensor), которые ожидают определенного события.

4. Scheduler (Планировщик)

Планировщик отвечает за определение времени и порядка запуска задач в DAG согласно расписанию. Он отслеживает готовность задач к запуску и инициирует их выполнение.

5. Executor (Исполнитель)

Исполнитель отвечает за непосредственный запуск задач. В зависимости от выбранного типа (например, LocalExecutor, CeleryExecutor, KubernetesExecutor) задачи могут выполняться локально, в распределённой среде или в Kubernetes-кластере.

6. Web Server (Веб-сервер) / WebUI

Веб-интерфейс Airflow позволяет пользователям управлять DAG'ами, запускать их вручную, отслеживать статус задач, просматривать логи и управлять настройками.

7. Metadata Database (База данных метаданных)

Центральное хранилище для всей служебной информации: статусов DAG и задач, расписаний, переменных, соединений и других метаданных. Обычно используется СУБД, совместимая с SQLAlchemy (PostgreSQL, MySQL и др.).

8. Worker (Рабочий процесс)

В распределённых конфигурациях (например, с CeleryExecutor) отдельные процессы-воркеры отвечают за выполнение задач, получая их от исполнителя.

Дополнительные компоненты
Sensor (Сенсор): специальный вид операторов, которые «ждут» наступления определённого события.

Connections (Соединения): объекты для хранения данных о подключениях к внешним системам (БД, API и т.д.).

Variables (Переменные): механизм хранения пользовательских настроек и конфигураций.


![image](https://github.com/user-attachments/assets/c8d8b4ea-55a4-48f6-920d-8ffb41ca0a76)

*************************************************************************************************************************************************
# 2) Какие операторы Airflow вы знаете?

В Apache Airflow существует множество операторов, которые представляют собой строительные блоки для определения задач в DAG. Вот основные и часто используемые операторы:

**PythonOperator** - позволяет запускать произвольные функции на Python. Один из самых популярных операторов благодаря гибкости.

**BashOperator** - выполняет команды bash или shell-скрипты. Используется для простых команд и скриптов.

**PostgresOperator** - выполняет SQL-запросы в базе данных PostgreSQL, используя подключение Airflow.

**SSHOperator** - запускает команды на удалённом сервере по SSH с использованием заранее настроенного подключения.

**EmailOperator** - отправляет электронные письма (часто используется для уведомлений).

**HttpOperator** - выполняет HTTP-запросы к API или веб-сервисам.

**DockerOperator** - запускает задачи внутри Docker-контейнеров.

**S3FileTransformOperator** - для работы с файлами в S3, например, преобразование данных.

**Transfer Operators (например, S3ToRedshiftOperator)** - предназначены для перемещения данных между системами, например, из S3 в Amazon Redshift.

**SlackAPIOperator** - отправка сообщений в Slack через API.

**SQLExecuteQueryOperator** - выполнение SQL-запросов в различных СУБД.

**YQExecuteQueryOperator** - оператор для выполнения запросов в Yandex Query.

Кроме того, существуют специализированные операторы от провайдеров, например, для работы с Airbyte API (AirbyteGeneralOperator) и другие, расширяющие функциональность Airflow.

Также стоит отметить, что начиная с Airflow 2.0 появилась возможность создавать операторы с помощью декораторов Python, что упрощает их использование и передачу данных между задачами.

Таким образом, Airflow предоставляет широкий набор операторов для различных задач: от запуска кода и команд до интеграции с базами данных, облачными сервисами и системами передачи данных. Это делает его универсальным инструментом для оркестрации рабочих процессов.
*************************************************************************************************************************************************
# 3) что такое сенсер в airflow и для чего он нужен?
Сенсор (sensor) в Apache Airflow - это особый тип оператора, задача которого - ожидать наступления определённого события или выполнения условия перед продолжением выполнения цепочки задач (DAG). Сенсоры проверяют, произошло ли нужное событие, например, появление файла, завершение задачи в другом DAG, доступность API или выполнение SQL-запроса, и только после этого позволяют запускать последующие задачи.

Основные особенности сенсоров в Airflow:
Сенсор постоянно проверяет условие с заданным интервалом (poke_interval).

Если условие не выполнено, сенсор "засыпает" и повторяет проверку.

Сенсор считается успешным, когда условие выполняется (возвращается True).

Сенсоры бывают разных типов, например:

FileSensor - ждёт появления файла.

HttpSensor - ждёт ответа от API.

ExternalTaskSensor - ждёт завершения задачи в другом DAG.

SqlSensor - ждёт появления данных в базе.

PythonSensor - ждёт выполнения пользовательского условия.

Режимы работы сенсоров:
poke (по умолчанию) - сенсор занимает рабочий слот на всё время ожидания, периодически проверяя условие.

reschedule - сенсор освобождает слот между проверками, что эффективнее при длительном ожидании.

smart sensor (устаревший, заменён на deferrable operators) - интеллектуальный режим для оптимизации ожидания.

Для чего нужен сенсор?
Сенсоры используются для синхронизации и управления потоками данных и задач в Airflow, когда необходимо дождаться какого-то события, прежде чем продолжить обработку. Например:

Ожидание загрузки файла в облачное хранилище для последующей обработки.

Ожидание завершения задачи в другом DAG для организации зависимостей между конвейерами.

Ожидание появления данных в базе или доступности внешнего API.

Автоматизация процессов ETL/ELT с поэтапной загрузкой данных в хранилище.

Таким образом, сенсор - это инструмент для реализации событийно-ориентированного управления задачами в Airflow, позволяющий сделать DAG'и более гибкими и управляемыми по условиям внешних событий и данных.
*************************************************************************************************************************************************
# 4)Таска в AirFlow упала с ошибкой, как сделать так, чтобы не смотря на ошибку, следующая таска запустилась?
Чтобы следующая задача в Airflow запускалась независимо от того, упала ли предыдущая задача с ошибкой, нужно изменить правило триггера (trigger rule) для этой следующей задачи.

По умолчанию в Airflow для запуска задачи используется правило all_success, которое требует успешного выполнения всех upstream-задач. Если предыдущая задача упала, следующая не запустится и получит статус upstream_failed.

Чтобы запустить следующую задачу несмотря на ошибку предыдущей, нужно установить для неё правило триггера:

all_done - задача запустится, когда все upstream-задачи завершатся в любом состоянии (успех, ошибка, пропуск).

Также можно использовать другие правила, например, one_failed (запускается, если хотя бы одна upstream-задача упала) или none_failed (запускается, если ни одна upstream-задача не упала).

Пример кода для оператора с правилом триггера all_done:

python
from airflow.operators.dummy import DummyOperator

task2 = DummyOperator(
    task_id='task2',
    trigger_rule='all_done',
    dag=dag,
)
*************************************************************************************************************************************************
# 5) Как в AirFlow в зависимости от условия, продолжить обработку по нужной ветки ДАГа?

В Apache Airflow для организации условного ветвления DAG и продолжения обработки по нужной ветке в зависимости от условия используется оператор BranchPythonOperator.

Как работает BranchPythonOperator:
В качестве входа он принимает функцию Python, которая реализует вашу логику выбора ветки.

Эта функция должна возвращать идентификатор (task_id) одной или нескольких задач - веток, которые должны быть выполнены.

Все остальные ветки, не выбранные функцией, будут автоматически помечены как пропущенные (skipped).

Если в какой-то ветке не требуется выполнять задачи, можно использовать EmptyOperator (ранее DummyOperator) для заполнения этой ветки.

Пример использования:

![image](https://github.com/user-attachments/assets/f189dfd4-b7b2-4ad5-bc56-394870e1970b)



Важные моменты:
Если после ветвления есть задачи, которые должны выполняться независимо от выбранной ветки (например, задача "слияния"), для них нужно изменить параметр trigger_rule на none_failed_min_one_success или all_done. По умолчанию стоит all_success, и если upstream-задачи пропущены, downstream-задача не запустится.

Для простого пропуска веток можно использовать оператор ShortCircuitOperator, который прекращает выполнение всех downstream-задач, если условие возвращает False.

Итог:
Используйте BranchPythonOperator для выбора ветки по условию.

Возвращайте из функции task_id нужной ветки.

Пропущенные ветки автоматически не выполняются.

Для объединения веток настройте правильное правило триггера downstream-задач.

Это самый распространённый и удобный способ реализовать условное ветвление в DAG Airflow.
*************************************************************************************************************************************************
# 6) Что такое XCOM?
XCom (сокращение от Cross-Communication) в Apache Airflow - это механизм обмена небольшими данными между задачами внутри одного DAG. Он позволяет задачам, которые изначально изолированы и могут выполняться на разных машинах, «разговаривать» друг с другом, передавая параметры и результаты.

Основные характеристики XCom:
XCom хранит данные в виде пар ключ-значение, связанных с конкретной задачей (task_id) и DAG (dag_id).

Передача данных происходит с помощью методов xcom_push() (записать данные) и xcom_pull() (получить данные).

Многие операторы автоматически отправляют результаты своей работы в XCom (например, PythonOperator возвращает значение функции).

XCom предназначен для небольших объёмов данных, так как данные сохраняются в базе метаданных Airflow (ограничения по размеру зависят от СУБД, например, для MySQL - 64 КБ).

XCom используется для передачи промежуточных результатов между задачами без необходимости внешнего хранилища.

Пример использования:

![image](https://github.com/user-attachments/assets/0bbdbdc3-1085-4fad-abd6-8ccf482ae739)


Таким образом, XCom - это встроенный в Airflow механизм для передачи небольших данных между задачами одного рабочего процесса, позволяющий строить более гибкие и динамичные DAG’и без использования внешних систем для обмена данными

*************************************************************************************************************************************************

# 7) Какие базы данных используется в Airflow?
В Apache Airflow для хранения метаданных (информации о DAG, задачах, их состоянии, расписании и т.д.) используются реляционные базы данных. По умолчанию Airflow поставляется с SQLite, которая подходит только для разработки и тестирования, но не для промышленного использования из-за ограничений по параллельному исполнению задач.

Для продакшен-среды рекомендуются следующие базы данных:

PostgreSQL (версии 12–16) - наиболее часто используемая и рекомендуемая база для Airflow, обеспечивает стабильную работу и поддержку параллелизма.

MySQL (начиная с версии 8.0) - также поддерживается, но с некоторыми ограничениями и особенностями, в частности MariaDB не рекомендуется из-за отличий в индексации и совместимости.

Airflow использует ORM-библиотеку SQLAlchemy для взаимодействия с базой данных метаданных, что позволяет легко менять СУБД, задавая строку подключения в конфигурации.

Кроме базы метаданных, Airflow интегрируется с множеством других баз данных и систем хранения данных для выполнения задач и работы с данными, например, Hadoop, Spark, MongoDB, Redis, Apache Hive, S3 и др..

Итог:
Для хранения метаданных Airflow по умолчанию - SQLite (для тестов).

Для продакшена - PostgreSQL или MySQL 8.0.

Для взаимодействия с данными в задачах - множество других СУБД и хранилищ.

Это обеспечивает гибкость и масштабируемость Airflow в разных сценариях использования.
*************************************************************************************************************************************************
# 8) Чем отличается Celery Executor и local executor?

Основные отличия между Celery Executor и Local Executor в Apache Airflow заключаются в архитектуре, масштабируемости и способе выполнения задач:


![image](https://github.com/user-attachments/assets/c6c9c332-c389-41d9-9373-96146d3629e2)

Кратко:
Local Executor - простой локальный исполнитель, запускает задачи параллельно на одной машине, подходит для небольших нагрузок и простых установок.

Celery Executor - распределённый исполнитель, использующий брокер сообщений и кластер worker-ов, обеспечивает масштабируемость и отказоустойчивость для больших и распределённых систем.

Выбор зависит от объёма задач и инфраструктуры: если нужно масштабировать выполнение задач на несколько серверов и обеспечить отказоустойчивость - выбирайте Celery Executor, если же задачи небольшие и всё можно выполнить на одном сервере - достаточно Local Executor.


*************************************************************************************************************************************************



